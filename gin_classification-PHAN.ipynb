{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-05T16:32:48.146430Z",
     "start_time": "2025-08-05T16:32:48.069844Z"
    }
   },
   "source": [
    "from collections import defaultdict\n",
    "import joblib\n",
    "import optuna.importance\n",
    "# autoimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:32:53.535468Z",
     "start_time": "2025-08-05T16:32:48.217438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.gnn_models import *\n",
    "%aimport src.gnn_models"
   ],
   "id": "1d4b298c2b66c61b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:32:53.623282Z",
     "start_time": "2025-08-05T16:32:53.548167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROCESSED_DATA_DIR = r\"E:\\gnn_data\\processed_step_data_global_features\"\n",
    "# PROCESSED_DATA_DIR = r\"E:\\gnn_data\\pyg_data_v2\"\n",
    "dataset = FastSTEPDataset(PROCESSED_DATA_DIR, start_index=0)\n",
    "get_dataset_stats(dataset)"
   ],
   "id": "35b087fe666c80cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast dataset loaded:\n",
      "- Total samples: 63043\n",
      "- Processed successfully: 63043\n",
      "- Failed processing: 54\n",
      "Label counts in dataset:\n",
      "Label 1: 40434 instances\n",
      "Label 0: 22609 instances\n",
      "Label 1: 64.14% of total instances\n",
      "Label 0: 35.86% of total instances\n",
      "Class weights for loss function: [1.5591581342434584, 2.7884028484231944]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:32:53.636740Z",
     "start_time": "2025-08-05T16:32:53.630651Z"
    }
   },
   "cell_type": "code",
   "source": "dataset[0]",
   "id": "7cd98b55c66799ca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[405, 7], edge_index=[2, 948], y=[1], global_features=[1, 14])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:45:01.023462Z",
     "start_time": "2025-08-05T16:45:00.987115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = GINCombined(\n",
    "    input_features=dataset[0].x.shape[1],\n",
    "    global_feature_dim=dataset[0].global_features.shape[1],\n",
    "    embedding_dim=16,\n",
    "    hidden_sizes=[256, 256],\n",
    "    conv_dropout_rate=0.1,\n",
    "    classifier_dropout_rate=0.1,\n",
    "    use_layer_norm=True,\n",
    "    pool_hidden_size=128\n",
    ")"
   ],
   "id": "9c0c461f5e1c6359",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Dynamic GIN model:\n",
      "- Input features: 7\n",
      "- Number of hidden layers: 2\n",
      "- Hidden layer sizes: [256, 256]\n",
      "- Output classes: 2\n",
      "- Convolution dropout rate: 0.1\n",
      "- Classifier dropout rate: 0.1\n",
      "- Layer normalization: True\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T13:12:54.615987Z",
     "start_time": "2025-08-02T13:12:54.611488Z"
    }
   },
   "cell_type": "code",
   "source": "all_history = {}",
   "id": "d675ebabfc32be37",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T16:45:07.321750Z",
     "start_time": "2025-08-05T16:45:04.534220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(1):\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "        model_save_path = \"gin_model_combined.pth\"\n",
    "        history_save_path = \"gin_model_combined_training_history.pkl\"\n",
    "        if Path(model_save_path).exists():\n",
    "            print(f\"Loading model from {model_save_path}\")\n",
    "            model.load_state_dict(torch.load(model_save_path))\n",
    "        else:\n",
    "            print(f\"Model file {model_save_path} does not exist. Initializing a new model.\")\n",
    "        # save training history\n",
    "\n",
    "        with open(history_save_path, \"rb\") as f:\n",
    "            all_history = joblib.load(f)\n",
    "        trained_model, history = simple_train_model_v3(\n",
    "            dataset,\n",
    "            gnn_model=model,\n",
    "            num_epochs=50,\n",
    "            batch_size=12,\n",
    "            learning_rate=0.0001,\n",
    "            start_index=0,\n",
    "            num_graphs_to_use=63000,\n",
    "        )\n",
    "        for key in history.keys():\n",
    "            if not key in all_history:\n",
    "                all_history[key] = []\n",
    "            all_history[key].extend(history[key])\n",
    "        all_history[\"epoch\"] = list(range(1, len(all_history[\"epoch\"]) + 1))\n",
    "        with open(history_save_path, \"wb\") as f:\n",
    "            joblib.dump(all_history, f)\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training epoch {i}: {e}\")\n",
    "        continue"
   ],
   "id": "65544adba1a874ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from gin_model_combined.pth\n",
      "Label 1: 40414 instances\n",
      "Label 0: 22586 instances\n",
      "Label 1: 64.15% of total instances\n",
      "Label 0: 35.85% of total instances\n",
      "Class weights: tensor([2.7893, 1.5589], device='cuda:0')\n",
      "Splitting dataset into train and validation sets\n",
      "Train samples: 50400\n",
      "Validation samples: 12600\n",
      "\n",
      "Starting training for 50 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]:   0%|          | 0/4200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([65527, 7])\n",
      "edge_index shape: torch.Size([2, 146120])\n",
      "batch shape: torch.Size([65527])\n",
      "global_features shape: torch.Size([12, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]:   0%|          | 3/4200 [00:00<11:48,  5.93it/s, loss=0.1460, acc=86.11%, f1=0.8621]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([63817, 7])\n",
      "edge_index shape: torch.Size([2, 145736])\n",
      "batch shape: torch.Size([63817])\n",
      "global_features shape: torch.Size([12, 14])\n",
      "x shape: torch.Size([95164, 7])\n",
      "edge_index shape: torch.Size([2, 227586])\n",
      "batch shape: torch.Size([95164])\n",
      "global_features shape: torch.Size([12, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]:   0%|          | 5/4200 [00:00<09:54,  7.05it/s, loss=0.5315, acc=88.33%, f1=0.8838]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([117263, 7])\n",
      "edge_index shape: torch.Size([2, 274114])\n",
      "batch shape: torch.Size([117263])\n",
      "global_features shape: torch.Size([12, 14])\n",
      "x shape: torch.Size([51274, 7])\n",
      "edge_index shape: torch.Size([2, 120934])\n",
      "batch shape: torch.Size([51274])\n",
      "global_features shape: torch.Size([12, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]:   0%|          | 7/4200 [00:01<08:51,  7.90it/s, loss=0.1863, acc=86.90%, f1=0.8693]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([36958, 7])\n",
      "edge_index shape: torch.Size([2, 85474])\n",
      "batch shape: torch.Size([36958])\n",
      "global_features shape: torch.Size([12, 14])\n",
      "x shape: torch.Size([85232, 7])\n",
      "edge_index shape: torch.Size([2, 193238])\n",
      "batch shape: torch.Size([85232])\n",
      "global_features shape: torch.Size([12, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]:   0%|          | 8/4200 [00:01<08:29,  8.24it/s, loss=0.6495, acc=86.11%, f1=0.8617]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([31701, 7])\n",
      "edge_index shape: torch.Size([2, 74712])\n",
      "batch shape: torch.Size([31701])\n",
      "global_features shape: torch.Size([12, 14])\n",
      "x shape: torch.Size([32789, 7])\n",
      "edge_index shape: torch.Size([2, 76918])\n",
      "batch shape: torch.Size([32789])\n",
      "global_features shape: torch.Size([12, 14])\n",
      "x shape: torch.Size([154491, 7])\n",
      "edge_index shape: torch.Size([2, 372812])\n",
      "batch shape: torch.Size([154491])\n",
      "global_features shape: torch.Size([12, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]:   0%|          | 12/4200 [00:01<07:40,  9.09it/s, loss=0.1793, acc=87.18%, f1=0.8726]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([33555, 7])\n",
      "edge_index shape: torch.Size([2, 75390])\n",
      "batch shape: torch.Size([33555])\n",
      "global_features shape: torch.Size([12, 14])\n",
      "x shape: torch.Size([34735, 7])\n",
      "edge_index shape: torch.Size([2, 80012])\n",
      "batch shape: torch.Size([34735])\n",
      "global_features shape: torch.Size([12, 14])\n",
      "x shape: torch.Size([15593, 7])\n",
      "edge_index shape: torch.Size([2, 36416])\n",
      "batch shape: torch.Size([15593])\n",
      "global_features shape: torch.Size([12, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]:   0%|          | 14/4200 [00:01<09:09,  7.62it/s, loss=0.2030, acc=86.90%, f1=0.8698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([50032, 7])\n",
      "edge_index shape: torch.Size([2, 112184])\n",
      "batch shape: torch.Size([50032])\n",
      "global_features shape: torch.Size([12, 14])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 15\u001B[39m\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(history_save_path, \u001B[33m\"\u001B[39m\u001B[33mrb\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m     14\u001B[39m     all_history = joblib.load(f)\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m trained_model, history = simple_train_model_v3(\n\u001B[32m     16\u001B[39m     dataset,\n\u001B[32m     17\u001B[39m     gnn_model=model,\n\u001B[32m     18\u001B[39m     num_epochs=\u001B[32m50\u001B[39m,\n\u001B[32m     19\u001B[39m     batch_size=\u001B[32m12\u001B[39m,\n\u001B[32m     20\u001B[39m     learning_rate=\u001B[32m0.0001\u001B[39m,\n\u001B[32m     21\u001B[39m     start_index=\u001B[32m0\u001B[39m,\n\u001B[32m     22\u001B[39m     num_graphs_to_use=\u001B[32m63000\u001B[39m,\n\u001B[32m     23\u001B[39m )\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m history.keys():\n\u001B[32m     25\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m all_history:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<string>:56\u001B[39m, in \u001B[36msimple_train_model_v3\u001B[39m\u001B[34m(dataset, gnn_model, num_epochs, batch_size, learning_rate, start_index, num_graphs_to_use)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machine-learning\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[32m   1182\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[32m   1183\u001B[39m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[32m   1184\u001B[39m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machine-learning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    705\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    706\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    707\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m708\u001B[39m data = \u001B[38;5;28mself\u001B[39m._next_data()\n\u001B[32m    709\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    710\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    711\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    712\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    713\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    714\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machine-learning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    762\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    763\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m764\u001B[39m     data = \u001B[38;5;28mself\u001B[39m._dataset_fetcher.fetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    765\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    766\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machine-learning\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     48\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.auto_collation:\n\u001B[32m     49\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.dataset, \u001B[33m\"\u001B[39m\u001B[33m__getitems__\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset.__getitems__:\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     52\u001B[39m         data = [\u001B[38;5;28mself\u001B[39m.dataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machine-learning\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001B[39m, in \u001B[36mSubset.__getitems__\u001B[39m\u001B[34m(self, indices)\u001B[39m\n\u001B[32m    418\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset.__getitems__([\u001B[38;5;28mself\u001B[39m.indices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m    419\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m420\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m.dataset[\u001B[38;5;28mself\u001B[39m.indices[idx]] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machine-learning\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001B[39m, in \u001B[36m<listcomp>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m    418\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset.__getitems__([\u001B[38;5;28mself\u001B[39m.indices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m    419\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m420\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m.dataset[\u001B[38;5;28mself\u001B[39m.indices[idx]] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Documents\\Study\\PythonProjects\\manufacturing-tech-gnn\\src\\gnn_models.py:76\u001B[39m, in \u001B[36mFastSTEPDataset.__getitem__\u001B[39m\u001B[34m(self, index)\u001B[39m\n\u001B[32m     73\u001B[39m processed_path = file_info[\u001B[33m'\u001B[39m\u001B[33mprocessed_path\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m     75\u001B[39m \u001B[38;5;66;03m# Load the pre-processed PyTorch Geometric data\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m76\u001B[39m data = torch.load(processed_path, weights_only=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m     77\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.start_index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(data,\n\u001B[32m     78\u001B[39m                                             \u001B[33m'\u001B[39m\u001B[33mx\u001B[39m\u001B[33m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m data.x \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     79\u001B[39m     data = data.clone()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machine-learning\\Lib\\site-packages\\torch\\serialization.py:1425\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[39m\n\u001B[32m   1422\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args.keys():\n\u001B[32m   1423\u001B[39m     pickle_load_args[\u001B[33m\"\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1425\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m _open_file_like(f, \u001B[33m\"\u001B[39m\u001B[33mrb\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[32m   1426\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[32m   1427\u001B[39m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[32m   1428\u001B[39m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[32m   1429\u001B[39m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[32m   1430\u001B[39m         orig_position = opened_file.tell()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machine-learning\\Lib\\site-packages\\torch\\serialization.py:751\u001B[39m, in \u001B[36m_open_file_like\u001B[39m\u001B[34m(name_or_buffer, mode)\u001B[39m\n\u001B[32m    749\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[32m    750\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[32m--> \u001B[39m\u001B[32m751\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m _open_file(name_or_buffer, mode)\n\u001B[32m    752\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    753\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mw\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machine-learning\\Lib\\site-packages\\torch\\serialization.py:732\u001B[39m, in \u001B[36m_open_file.__init__\u001B[39m\u001B[34m(self, name, mode)\u001B[39m\n\u001B[32m    731\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[32m--> \u001B[39m\u001B[32m732\u001B[39m     \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m(\u001B[38;5;28mopen\u001B[39m(name, mode))\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T23:18:41.084422Z",
     "start_time": "2025-08-02T23:18:41.073429Z"
    }
   },
   "cell_type": "code",
   "source": "all_history",
   "id": "d6187407dca0b281",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.6210318155693156,\n",
       "  0.5884338162484624,\n",
       "  0.5807581246058856,\n",
       "  0.5784128614000621,\n",
       "  0.5751571377641743,\n",
       "  0.5726831243905638,\n",
       "  0.5708053694754129,\n",
       "  0.5699995922900382,\n",
       "  0.567597364683946,\n",
       "  0.5667620221028725,\n",
       "  0.5674510450155608,\n",
       "  0.5656477987482434,\n",
       "  0.5645915722554283,\n",
       "  0.5617489199074251,\n",
       "  0.5628458397702447,\n",
       "  0.5620067406915837,\n",
       "  0.559581496432601,\n",
       "  0.5604556760866017,\n",
       "  0.5597009777730064,\n",
       "  0.5600156294989089,\n",
       "  0.5582568416744471,\n",
       "  0.5587369639506298,\n",
       "  0.557835575489416,\n",
       "  0.5562462923622558,\n",
       "  0.5561987818192159,\n",
       "  0.5555666007083796,\n",
       "  0.5553133543775904,\n",
       "  0.5549816019389601,\n",
       "  0.5563397216601741,\n",
       "  0.5559190037278902,\n",
       "  0.5546640211006715,\n",
       "  0.5552630360176166,\n",
       "  0.5537093390675173,\n",
       "  0.5547770561242388,\n",
       "  0.555516376777419,\n",
       "  0.5539051185841007,\n",
       "  0.5538902622958024,\n",
       "  0.5519740365392395,\n",
       "  0.5521404327993237,\n",
       "  0.5517584696465305,\n",
       "  0.5518727227344754,\n",
       "  0.5511733574863701,\n",
       "  0.5518009466527118,\n",
       "  0.6181215317653758,\n",
       "  0.5722680032608055,\n",
       "  0.5479308696552402,\n",
       "  0.5346723513615628,\n",
       "  0.5243435534302677,\n",
       "  0.5171552996328544,\n",
       "  0.5114997099552836,\n",
       "  0.505899496663894,\n",
       "  0.5009492526886364,\n",
       "  0.4945863046505976,\n",
       "  0.48987094711201884,\n",
       "  0.48478461902899045,\n",
       "  0.4829292737364414,\n",
       "  0.47804475183554346,\n",
       "  0.4760447183388862,\n",
       "  0.47438328008877023,\n",
       "  0.4725932778602111,\n",
       "  0.46805687105017046,\n",
       "  0.46640215630931336,\n",
       "  0.4651924544448654,\n",
       "  0.46480930694866746,\n",
       "  0.4617918827664107,\n",
       "  0.4589667790693541,\n",
       "  0.4568108919511239,\n",
       "  0.45515291863741975,\n",
       "  0.45356597095284434,\n",
       "  0.4517837915228059,\n",
       "  0.44991301265722583,\n",
       "  0.4475029505199442,\n",
       "  0.4456103379824864,\n",
       "  0.43915861447119997,\n",
       "  0.43342527880200316,\n",
       "  0.43282130153672327,\n",
       "  0.43190578747540714,\n",
       "  0.43121739843948964,\n",
       "  0.4291200297787076,\n",
       "  0.4259288933890916,\n",
       "  0.4266055408741037,\n",
       "  0.42467129763393174,\n",
       "  0.4247214144335261,\n",
       "  0.4236998637765646,\n",
       "  0.4217665595250825,\n",
       "  0.4220774382583442,\n",
       "  0.41913673330009693,\n",
       "  0.4186942967062905,\n",
       "  0.419221744249974,\n",
       "  0.41708137921988964,\n",
       "  0.41712641825544694,\n",
       "  0.4148375991501269,\n",
       "  0.4129839688574984,\n",
       "  0.41018452249023885,\n",
       "  0.4120628992592295,\n",
       "  0.41093877428078224,\n",
       "  0.40903858934928267,\n",
       "  0.40728333364285174,\n",
       "  0.4071782370426116,\n",
       "  0.40715595848680963,\n",
       "  0.40572553305487546,\n",
       "  0.4039811594961655,\n",
       "  0.40540473303269775,\n",
       "  0.40250388213743765,\n",
       "  0.40157625030726196,\n",
       "  0.39989467692605796,\n",
       "  0.40065042449516197,\n",
       "  0.3993194334262184,\n",
       "  0.398752625935844,\n",
       "  0.39671131277279487,\n",
       "  0.3967953309301464,\n",
       "  0.39725987376024324,\n",
       "  0.39468416565585707,\n",
       "  0.39557284557571015,\n",
       "  0.39047977674752476,\n",
       "  0.392195114663669,\n",
       "  0.3908888339304498,\n",
       "  0.3900999124569907],\n",
       " 'train_acc': [67.52380728721619,\n",
       "  70.80158591270447,\n",
       "  71.27976417541504,\n",
       "  71.46825194358826,\n",
       "  71.70237898826599,\n",
       "  71.72619104385376,\n",
       "  71.83730006217957,\n",
       "  72.00595140457153,\n",
       "  72.1666693687439,\n",
       "  72.34722375869751,\n",
       "  72.15476036071777,\n",
       "  72.51389026641846,\n",
       "  72.50000238418579,\n",
       "  72.67658710479736,\n",
       "  72.56547808647156,\n",
       "  72.46428728103638,\n",
       "  72.76785969734192,\n",
       "  72.78968095779419,\n",
       "  72.88095355033875,\n",
       "  73.05356860160828,\n",
       "  72.81944155693054,\n",
       "  72.74603247642517,\n",
       "  72.99603223800659,\n",
       "  72.98214435577393,\n",
       "  73.14682602882385,\n",
       "  73.16666841506958,\n",
       "  73.2797622680664,\n",
       "  72.96627163887024,\n",
       "  73.23015928268433,\n",
       "  73.33333492279053,\n",
       "  73.27777743339539,\n",
       "  73.24404716491699,\n",
       "  73.33928346633911,\n",
       "  73.3134925365448,\n",
       "  73.26388955116272,\n",
       "  73.4067440032959,\n",
       "  73.36111068725586,\n",
       "  73.53174686431885,\n",
       "  73.46230149269104,\n",
       "  73.4761893749237,\n",
       "  73.42658638954163,\n",
       "  73.37102890014648,\n",
       "  73.33333492279053,\n",
       "  67.65674352645874,\n",
       "  72.0119059085846,\n",
       "  73.64484071731567,\n",
       "  74.48015809059143,\n",
       "  74.96428489685059,\n",
       "  75.80754160881042,\n",
       "  76.13492012023926,\n",
       "  76.4563500881195,\n",
       "  76.74008011817932,\n",
       "  77.21230387687683,\n",
       "  77.52777934074402,\n",
       "  77.70634889602661,\n",
       "  77.83135175704956,\n",
       "  78.125,\n",
       "  78.42261791229248,\n",
       "  78.1765878200531,\n",
       "  78.50793600082397,\n",
       "  78.74206304550171,\n",
       "  78.6210298538208,\n",
       "  78.71428728103638,\n",
       "  78.9722204208374,\n",
       "  79.15872931480408,\n",
       "  79.2202353477478,\n",
       "  79.32738065719604,\n",
       "  79.51785922050476,\n",
       "  79.38690185546875,\n",
       "  79.7777771949768,\n",
       "  79.59523797035217,\n",
       "  80.0595223903656,\n",
       "  79.82738018035889,\n",
       "  80.09523749351501,\n",
       "  80.30158877372742,\n",
       "  80.49007654190063,\n",
       "  80.51190376281738,\n",
       "  80.54563403129578,\n",
       "  80.63690662384033,\n",
       "  80.6170642375946,\n",
       "  80.63690662384033,\n",
       "  80.71230053901672,\n",
       "  80.86110949516296,\n",
       "  80.92460036277771,\n",
       "  81.0099184513092,\n",
       "  80.97420930862427,\n",
       "  81.03373050689697,\n",
       "  81.11904859542847,\n",
       "  81.1984121799469,\n",
       "  81.23412728309631,\n",
       "  81.32737874984741,\n",
       "  81.53769969940186,\n",
       "  81.4801573753357,\n",
       "  81.58531785011292,\n",
       "  81.4563512802124,\n",
       "  81.65476322174072,\n",
       "  81.83928728103638,\n",
       "  81.84523582458496,\n",
       "  81.78372979164124,\n",
       "  81.71229958534241,\n",
       "  81.94246292114258,\n",
       "  82.06349015235901,\n",
       "  81.9007933139801,\n",
       "  82.1587324142456,\n",
       "  82.12896585464478,\n",
       "  82.25396871566772,\n",
       "  82.16666579246521,\n",
       "  82.19047784805298,\n",
       "  82.31745958328247,\n",
       "  82.52381086349487,\n",
       "  82.54563212394714,\n",
       "  82.50595331192017,\n",
       "  82.49008059501648,\n",
       "  82.48412609100342,\n",
       "  82.85515904426575,\n",
       "  82.6369047164917,\n",
       "  82.66269564628601,\n",
       "  82.83928632736206],\n",
       " 'train_f1': [0.6777742505073547,\n",
       "  0.7109556198120117,\n",
       "  0.7156367897987366,\n",
       "  0.7172452211380005,\n",
       "  0.7195045948028564,\n",
       "  0.7197862863540649,\n",
       "  0.7209300994873047,\n",
       "  0.7225900888442993,\n",
       "  0.7242431044578552,\n",
       "  0.7260112762451172,\n",
       "  0.7240931391716003,\n",
       "  0.7274221181869507,\n",
       "  0.7273958921432495,\n",
       "  0.7291353344917297,\n",
       "  0.7278651595115662,\n",
       "  0.7269339561462402,\n",
       "  0.7298954725265503,\n",
       "  0.7301081418991089,\n",
       "  0.7309705018997192,\n",
       "  0.7326716184616089,\n",
       "  0.7305809259414673,\n",
       "  0.7298421859741211,\n",
       "  0.7323664426803589,\n",
       "  0.732254147529602,\n",
       "  0.7337371110916138,\n",
       "  0.7339558601379395,\n",
       "  0.7350881099700928,\n",
       "  0.7320968508720398,\n",
       "  0.7345755696296692,\n",
       "  0.7355888485908508,\n",
       "  0.7349444627761841,\n",
       "  0.7348137497901917,\n",
       "  0.7355878353118896,\n",
       "  0.7353209853172302,\n",
       "  0.73491370677948,\n",
       "  0.7362960577011108,\n",
       "  0.7359175682067871,\n",
       "  0.737594723701477,\n",
       "  0.7369219064712524,\n",
       "  0.7370439767837524,\n",
       "  0.7364585399627686,\n",
       "  0.7361340522766113,\n",
       "  0.7356855273246765,\n",
       "  0.6785928606987,\n",
       "  0.723037838935852,\n",
       "  0.7397249341011047,\n",
       "  0.7481229305267334,\n",
       "  0.7530996203422546,\n",
       "  0.7611886858940125,\n",
       "  0.7645239233970642,\n",
       "  0.7678015828132629,\n",
       "  0.7705343961715698,\n",
       "  0.7751631140708923,\n",
       "  0.7782484292984009,\n",
       "  0.7799650430679321,\n",
       "  0.781194806098938,\n",
       "  0.7841362953186035,\n",
       "  0.7869951725006104,\n",
       "  0.7846848964691162,\n",
       "  0.7878962755203247,\n",
       "  0.7902292609214783,\n",
       "  0.7890461683273315,\n",
       "  0.7898858785629272,\n",
       "  0.7924604415893555,\n",
       "  0.7942640781402588,\n",
       "  0.7949001789093018,\n",
       "  0.7959547638893127,\n",
       "  0.797809898853302,\n",
       "  0.7964949607849121,\n",
       "  0.8003823161125183,\n",
       "  0.7986152768135071,\n",
       "  0.8031519651412964,\n",
       "  0.800815761089325,\n",
       "  0.8037245273590088,\n",
       "  0.8057370185852051,\n",
       "  0.8076519966125488,\n",
       "  0.8077928423881531,\n",
       "  0.8080638647079468,\n",
       "  0.8090380430221558,\n",
       "  0.8088057041168213,\n",
       "  0.8090484142303467,\n",
       "  0.8097931742668152,\n",
       "  0.8112536668777466,\n",
       "  0.8118631839752197,\n",
       "  0.8126640915870667,\n",
       "  0.8122657537460327,\n",
       "  0.8128648996353149,\n",
       "  0.813734769821167,\n",
       "  0.8145421147346497,\n",
       "  0.8149133920669556,\n",
       "  0.8157662153244019,\n",
       "  0.817864179611206,\n",
       "  0.8173450231552124,\n",
       "  0.818364143371582,\n",
       "  0.8170725107192993,\n",
       "  0.8190271854400635,\n",
       "  0.820838212966919,\n",
       "  0.8208925724029541,\n",
       "  0.820350706577301,\n",
       "  0.819542407989502,\n",
       "  0.8218929767608643,\n",
       "  0.8230209350585938,\n",
       "  0.82140052318573,\n",
       "  0.8239656686782837,\n",
       "  0.8237107396125793,\n",
       "  0.8248443007469177,\n",
       "  0.8239813446998596,\n",
       "  0.8242176175117493,\n",
       "  0.8254884481430054,\n",
       "  0.8274939060211182,\n",
       "  0.8277012705802917,\n",
       "  0.8273182511329651,\n",
       "  0.8271740674972534,\n",
       "  0.8271369934082031,\n",
       "  0.8307307362556458,\n",
       "  0.8286515474319458,\n",
       "  0.8288336992263794,\n",
       "  0.8305590152740479],\n",
       " 'train_precision': [0.6814112067222595,\n",
       "  0.7160106301307678,\n",
       "  0.7205156087875366,\n",
       "  0.7214367985725403,\n",
       "  0.7235301733016968,\n",
       "  0.7239236831665039,\n",
       "  0.7251684665679932,\n",
       "  0.7267891764640808,\n",
       "  0.7285886406898499,\n",
       "  0.7302906513214111,\n",
       "  0.7283560633659363,\n",
       "  0.7310868501663208,\n",
       "  0.7313346266746521,\n",
       "  0.7330352067947388,\n",
       "  0.7313644289970398,\n",
       "  0.7306108474731445,\n",
       "  0.7334363460540771,\n",
       "  0.7336388230323792,\n",
       "  0.7343946099281311,\n",
       "  0.7360589504241943,\n",
       "  0.7345464825630188,\n",
       "  0.7337842583656311,\n",
       "  0.7364116907119751,\n",
       "  0.736366331577301,\n",
       "  0.7374574542045593,\n",
       "  0.737730085849762,\n",
       "  0.7388829588890076,\n",
       "  0.7362099885940552,\n",
       "  0.7383214235305786,\n",
       "  0.7393039464950562,\n",
       "  0.7384340763092041,\n",
       "  0.7388152480125427,\n",
       "  0.7391545176506042,\n",
       "  0.738862156867981,\n",
       "  0.7386666536331177,\n",
       "  0.7399551272392273,\n",
       "  0.7397658824920654,\n",
       "  0.7413955926895142,\n",
       "  0.7407669425010681,\n",
       "  0.740848183631897,\n",
       "  0.7400319576263428,\n",
       "  0.7402912974357605,\n",
       "  0.7396470308303833,\n",
       "  0.6812901496887207,\n",
       "  0.7283071279525757,\n",
       "  0.746761679649353,\n",
       "  0.7557450532913208,\n",
       "  0.7616479992866516,\n",
       "  0.7685590982437134,\n",
       "  0.7723760604858398,\n",
       "  0.7761896848678589,\n",
       "  0.778552770614624,\n",
       "  0.7829925417900085,\n",
       "  0.785912275314331,\n",
       "  0.7873787879943848,\n",
       "  0.7885801792144775,\n",
       "  0.7917330265045166,\n",
       "  0.7941552996635437,\n",
       "  0.7924949526786804,\n",
       "  0.7953651547431946,\n",
       "  0.7978072166442871,\n",
       "  0.796694278717041,\n",
       "  0.7970878481864929,\n",
       "  0.7998007535934448,\n",
       "  0.8013885021209717,\n",
       "  0.8021794557571411,\n",
       "  0.8032107353210449,\n",
       "  0.8049150705337524,\n",
       "  0.8034874200820923,\n",
       "  0.8075070977210999,\n",
       "  0.8059482574462891,\n",
       "  0.8101935386657715,\n",
       "  0.8076272010803223,\n",
       "  0.8121147155761719,\n",
       "  0.8139657974243164,\n",
       "  0.8162525296211243,\n",
       "  0.8158870339393616,\n",
       "  0.8157577514648438,\n",
       "  0.8172070384025574,\n",
       "  0.8167338371276855,\n",
       "  0.8172871470451355,\n",
       "  0.8180360794067383,\n",
       "  0.8194392919540405,\n",
       "  0.8199318647384644,\n",
       "  0.8204562067985535,\n",
       "  0.8197637796401978,\n",
       "  0.8204349279403687,\n",
       "  0.8214815855026245,\n",
       "  0.8224468231201172,\n",
       "  0.8229455947875977,\n",
       "  0.8233429193496704,\n",
       "  0.8255804777145386,\n",
       "  0.8253966569900513,\n",
       "  0.8262835741043091,\n",
       "  0.8248665928840637,\n",
       "  0.8267917633056641,\n",
       "  0.8285269737243652,\n",
       "  0.8285510540008545,\n",
       "  0.8284664154052734,\n",
       "  0.8269513845443726,\n",
       "  0.829833447933197,\n",
       "  0.8304959535598755,\n",
       "  0.8287848830223083,\n",
       "  0.831468403339386,\n",
       "  0.8314840793609619,\n",
       "  0.831931471824646,\n",
       "  0.8310658931732178,\n",
       "  0.831308901309967,\n",
       "  0.8326875567436218,\n",
       "  0.834473192691803,\n",
       "  0.8346258401870728,\n",
       "  0.8343027830123901,\n",
       "  0.8342418670654297,\n",
       "  0.8343503475189209,\n",
       "  0.8374643325805664,\n",
       "  0.8359009027481079,\n",
       "  0.8356001377105713,\n",
       "  0.837196946144104],\n",
       " 'train_recall': [0.6752381324768066,\n",
       "  0.7080158591270447,\n",
       "  0.7127976417541504,\n",
       "  0.7146825194358826,\n",
       "  0.7170238494873047,\n",
       "  0.7172619104385376,\n",
       "  0.7183730006217957,\n",
       "  0.7200595140457153,\n",
       "  0.721666693687439,\n",
       "  0.7234722375869751,\n",
       "  0.7215476036071777,\n",
       "  0.7251389026641846,\n",
       "  0.7249999642372131,\n",
       "  0.7267658710479736,\n",
       "  0.7256547808647156,\n",
       "  0.7246428728103638,\n",
       "  0.7276785969734192,\n",
       "  0.7278968095779419,\n",
       "  0.7288094758987427,\n",
       "  0.7305357456207275,\n",
       "  0.7281944751739502,\n",
       "  0.7274603247642517,\n",
       "  0.7299603223800659,\n",
       "  0.7298214435577393,\n",
       "  0.7314682602882385,\n",
       "  0.7316666841506958,\n",
       "  0.7327976226806641,\n",
       "  0.7296626567840576,\n",
       "  0.7323015928268433,\n",
       "  0.7333333492279053,\n",
       "  0.7327777743339539,\n",
       "  0.7324404716491699,\n",
       "  0.7333928346633911,\n",
       "  0.733134925365448,\n",
       "  0.7326388955116272,\n",
       "  0.734067440032959,\n",
       "  0.7336111068725586,\n",
       "  0.7353174686431885,\n",
       "  0.7346230149269104,\n",
       "  0.7347618937492371,\n",
       "  0.7342658638954163,\n",
       "  0.7337103486061096,\n",
       "  0.7333333492279053,\n",
       "  0.6765674352645874,\n",
       "  0.720119059085846,\n",
       "  0.7364484071731567,\n",
       "  0.7448016405105591,\n",
       "  0.7496428489685059,\n",
       "  0.7580753564834595,\n",
       "  0.7613492012023926,\n",
       "  0.7645635008811951,\n",
       "  0.7674008011817932,\n",
       "  0.7721229791641235,\n",
       "  0.7752777338027954,\n",
       "  0.7770634889602661,\n",
       "  0.7783134579658508,\n",
       "  0.78125,\n",
       "  0.7842261791229248,\n",
       "  0.7817658185958862,\n",
       "  0.7850793600082397,\n",
       "  0.7874206304550171,\n",
       "  0.786210298538208,\n",
       "  0.7871428728103638,\n",
       "  0.789722204208374,\n",
       "  0.7915872931480408,\n",
       "  0.7922024130821228,\n",
       "  0.7932738065719604,\n",
       "  0.7951785326004028,\n",
       "  0.7938690781593323,\n",
       "  0.7977777719497681,\n",
       "  0.795952320098877,\n",
       "  0.8005952835083008,\n",
       "  0.7982738018035889,\n",
       "  0.8009524345397949,\n",
       "  0.8030158281326294,\n",
       "  0.8049007654190063,\n",
       "  0.8051190376281738,\n",
       "  0.8054563403129578,\n",
       "  0.8063690662384033,\n",
       "  0.8061705827713013,\n",
       "  0.8063690662384033,\n",
       "  0.807123064994812,\n",
       "  0.8086111545562744,\n",
       "  0.8092460632324219,\n",
       "  0.8100992441177368,\n",
       "  0.8097420930862427,\n",
       "  0.8103373050689697,\n",
       "  0.8111904859542847,\n",
       "  0.811984121799469,\n",
       "  0.8123413324356079,\n",
       "  0.8132737874984741,\n",
       "  0.8153769969940186,\n",
       "  0.8148015737533569,\n",
       "  0.8158531785011292,\n",
       "  0.814563512802124,\n",
       "  0.8165476322174072,\n",
       "  0.8183928728103638,\n",
       "  0.8184523582458496,\n",
       "  0.8178372979164124,\n",
       "  0.8171229958534241,\n",
       "  0.8194246292114258,\n",
       "  0.8206349611282349,\n",
       "  0.819007933139801,\n",
       "  0.8215872645378113,\n",
       "  0.8212896585464478,\n",
       "  0.8225396871566772,\n",
       "  0.8216665983200073,\n",
       "  0.8219047784805298,\n",
       "  0.8231745958328247,\n",
       "  0.8252381086349487,\n",
       "  0.8254563808441162,\n",
       "  0.8250595331192017,\n",
       "  0.8249008059501648,\n",
       "  0.8248412609100342,\n",
       "  0.8285515308380127,\n",
       "  0.826369047164917,\n",
       "  0.8266269564628601,\n",
       "  0.8283928632736206],\n",
       " 'train_auroc': [0.7121137380599976,\n",
       "  0.7560858726501465,\n",
       "  0.7644674777984619,\n",
       "  0.7671784162521362,\n",
       "  0.7703970670700073,\n",
       "  0.7728971242904663,\n",
       "  0.7749463319778442,\n",
       "  0.7754534482955933,\n",
       "  0.7781984806060791,\n",
       "  0.7786922454833984,\n",
       "  0.7786367535591125,\n",
       "  0.7811282277107239,\n",
       "  0.7814779281616211,\n",
       "  0.7838438153266907,\n",
       "  0.7822803258895874,\n",
       "  0.7836520671844482,\n",
       "  0.7856801748275757,\n",
       "  0.7850638628005981,\n",
       "  0.7863763570785522,\n",
       "  0.7863855361938477,\n",
       "  0.7872097492218018,\n",
       "  0.7866004705429077,\n",
       "  0.7879530191421509,\n",
       "  0.7892966270446777,\n",
       "  0.7892892360687256,\n",
       "  0.7905092835426331,\n",
       "  0.7906571626663208,\n",
       "  0.7906020283699036,\n",
       "  0.7898246049880981,\n",
       "  0.790228009223938,\n",
       "  0.7909122705459595,\n",
       "  0.7907818555831909,\n",
       "  0.7915568351745605,\n",
       "  0.7913632988929749,\n",
       "  0.7901574373245239,\n",
       "  0.7916640043258667,\n",
       "  0.7921634912490845,\n",
       "  0.7938891649246216,\n",
       "  0.7928460836410522,\n",
       "  0.7935119867324829,\n",
       "  0.7933571338653564,\n",
       "  0.7940540313720703,\n",
       "  0.7941831350326538,\n",
       "  0.7148042321205139,\n",
       "  0.7735130786895752,\n",
       "  0.7970455884933472,\n",
       "  0.8088018894195557,\n",
       "  0.8173661231994629,\n",
       "  0.8237923383712769,\n",
       "  0.8283644914627075,\n",
       "  0.8331046104431152,\n",
       "  0.8364366292953491,\n",
       "  0.8413803577423096,\n",
       "  0.8442885875701904,\n",
       "  0.8479589223861694,\n",
       "  0.8493958711624146,\n",
       "  0.8530499935150146,\n",
       "  0.8542975187301636,\n",
       "  0.8557648658752441,\n",
       "  0.8566712141036987,\n",
       "  0.8596541881561279,\n",
       "  0.8607281446456909,\n",
       "  0.8618149757385254,\n",
       "  0.8625303506851196,\n",
       "  0.8640985488891602,\n",
       "  0.8661372661590576,\n",
       "  0.8673802614212036,\n",
       "  0.8686521053314209,\n",
       "  0.8692094087600708,\n",
       "  0.8707807064056396,\n",
       "  0.8711106777191162,\n",
       "  0.8730741739273071,\n",
       "  0.8742388486862183,\n",
       "  0.8784041404724121,\n",
       "  0.8812547922134399,\n",
       "  0.882179856300354,\n",
       "  0.8824050426483154,\n",
       "  0.8828530311584473,\n",
       "  0.8842165470123291,\n",
       "  0.8858306407928467,\n",
       "  0.8854715824127197,\n",
       "  0.8865152597427368,\n",
       "  0.8865789175033569,\n",
       "  0.8875203728675842,\n",
       "  0.8883652091026306,\n",
       "  0.888602614402771,\n",
       "  0.8896540403366089,\n",
       "  0.8902047872543335,\n",
       "  0.8899437189102173,\n",
       "  0.8911697864532471,\n",
       "  0.8910962343215942,\n",
       "  0.892035961151123,\n",
       "  0.8930933475494385,\n",
       "  0.8948301076889038,\n",
       "  0.8937585353851318,\n",
       "  0.8943955898284912,\n",
       "  0.8955830335617065,\n",
       "  0.8963800668716431,\n",
       "  0.8966192007064819,\n",
       "  0.896663248538971,\n",
       "  0.8978121280670166,\n",
       "  0.8982793092727661,\n",
       "  0.8974732160568237,\n",
       "  0.8991130590438843,\n",
       "  0.8993209600448608,\n",
       "  0.9003146886825562,\n",
       "  0.9005973935127258,\n",
       "  0.9007922410964966,\n",
       "  0.9012091159820557,\n",
       "  0.9024159908294678,\n",
       "  0.9023618698120117,\n",
       "  0.9025829434394836,\n",
       "  0.9030685424804688,\n",
       "  0.9032138586044312,\n",
       "  0.9052541255950928,\n",
       "  0.9044131636619568,\n",
       "  0.905128002166748,\n",
       "  0.9057642221450806],\n",
       " 'val_loss': [0.596539893441257,\n",
       "  0.5858472154679752,\n",
       "  0.5733526621191275,\n",
       "  0.5724855178026926,\n",
       "  0.5656510669560659,\n",
       "  0.5756007475938116,\n",
       "  0.5725136434677102,\n",
       "  0.5718633712686243,\n",
       "  0.5624630408556689,\n",
       "  0.5762390625547795,\n",
       "  0.5740445040804999,\n",
       "  0.5681013692063944,\n",
       "  0.5688157605841047,\n",
       "  0.5580598392585914,\n",
       "  0.5575140993509974,\n",
       "  0.5544952488087472,\n",
       "  0.5498261503449509,\n",
       "  0.5699986995934021,\n",
       "  0.56804953292367,\n",
       "  0.561976480171794,\n",
       "  0.5562445992586158,\n",
       "  0.5523188191155592,\n",
       "  0.563212760190169,\n",
       "  0.5609052146616437,\n",
       "  0.5514399492740631,\n",
       "  0.5578247559851124,\n",
       "  0.5652239870421943,\n",
       "  0.5681411215379124,\n",
       "  0.5623827839784679,\n",
       "  0.5588592386245728,\n",
       "  0.5607283966810931,\n",
       "  0.5654482019834575,\n",
       "  0.5643807511599291,\n",
       "  0.5593366386847837,\n",
       "  0.5635206376370929,\n",
       "  0.5610234581501711,\n",
       "  0.5610320928479944,\n",
       "  0.5617894109444959,\n",
       "  0.5675976228572074,\n",
       "  0.565313949886532,\n",
       "  0.5611847337512743,\n",
       "  0.5661712399196057,\n",
       "  0.55994923705856,\n",
       "  0.5725849320491155,\n",
       "  0.5514824251227436,\n",
       "  0.5488433681357474,\n",
       "  0.526767684210624,\n",
       "  0.5552528943972929,\n",
       "  0.5183357317461854,\n",
       "  0.5134477655660539,\n",
       "  0.5044423144630024,\n",
       "  0.5006990869307801,\n",
       "  0.4932958068645426,\n",
       "  0.49049959779495284,\n",
       "  0.5078203950396606,\n",
       "  0.4863870784853186,\n",
       "  0.4902589322821725,\n",
       "  0.49053879447103965,\n",
       "  0.4975657146903021,\n",
       "  0.48399475003814413,\n",
       "  0.5218528612259598,\n",
       "  0.5049817135335789,\n",
       "  0.4827453717057194,\n",
       "  0.47518569351129586,\n",
       "  0.5022080388886943,\n",
       "  0.49822015697668703,\n",
       "  0.4793327403707164,\n",
       "  0.4710649462203894,\n",
       "  0.47193437454778525,\n",
       "  0.4662449756317905,\n",
       "  0.46268338749096505,\n",
       "  0.4571248824787991,\n",
       "  0.45972138193835105,\n",
       "  0.4541661963718278,\n",
       "  0.45645857007020996,\n",
       "  0.46183122562510626,\n",
       "  0.4495234915188381,\n",
       "  0.4539442972484089,\n",
       "  0.4507317365351177,\n",
       "  0.45100068976481755,\n",
       "  0.45354729151441936,\n",
       "  0.44834862452177776,\n",
       "  0.4420224619550364,\n",
       "  0.4436396766702334,\n",
       "  0.4439883284483637,\n",
       "  0.4466343805832522,\n",
       "  0.4492715028425058,\n",
       "  0.44505324591483386,\n",
       "  0.4460906804885183,\n",
       "  0.44115517231680096,\n",
       "  0.4362701847368763,\n",
       "  0.43366374537348745,\n",
       "  0.43430282579291435,\n",
       "  0.4376200049831754,\n",
       "  0.43547435207735924,\n",
       "  0.4364365652132602,\n",
       "  0.4327819952652568,\n",
       "  0.4375544407396089,\n",
       "  0.43800091984016554,\n",
       "  0.4289712371215934,\n",
       "  0.43231887371767136,\n",
       "  0.429565164269436,\n",
       "  0.4349368460902146,\n",
       "  0.42823827206733683,\n",
       "  0.4307861865915003,\n",
       "  0.43153857057293254,\n",
       "  0.42497066458066307,\n",
       "  0.43768366900228317,\n",
       "  0.4259550865491231,\n",
       "  0.4254739434378488,\n",
       "  0.4211324400135449,\n",
       "  0.43495727854115623,\n",
       "  0.42074061707371757,\n",
       "  0.4248424705792041,\n",
       "  0.42289383184342155,\n",
       "  0.42694879402007374,\n",
       "  0.43991878958330266,\n",
       "  0.42926367932132314],\n",
       " 'val_acc': [71.50793671607971,\n",
       "  72.89682626724243,\n",
       "  72.96031713485718,\n",
       "  72.2857117652893,\n",
       "  72.19841480255127,\n",
       "  72.03968167304993,\n",
       "  70.44444680213928,\n",
       "  73.2619047164917,\n",
       "  72.19047546386719,\n",
       "  72.96031713485718,\n",
       "  72.2777783870697,\n",
       "  73.38888645172119,\n",
       "  74.0396797657013,\n",
       "  73.44444394111633,\n",
       "  73.83333444595337,\n",
       "  72.84126877784729,\n",
       "  74.48412775993347,\n",
       "  73.14285635948181,\n",
       "  73.21428656578064,\n",
       "  71.69841527938843,\n",
       "  74.03174638748169,\n",
       "  73.57142567634583,\n",
       "  73.3730137348175,\n",
       "  74.2222249507904,\n",
       "  73.52380752563477,\n",
       "  73.06349277496338,\n",
       "  73.10317754745483,\n",
       "  74.39682483673096,\n",
       "  73.82539510726929,\n",
       "  73.52380752563477,\n",
       "  73.23015928268433,\n",
       "  72.9285717010498,\n",
       "  73.43651056289673,\n",
       "  74.02380704879761,\n",
       "  74.13492202758789,\n",
       "  73.4761893749237,\n",
       "  73.53174686431885,\n",
       "  74.05555844306946,\n",
       "  73.84920716285706,\n",
       "  73.68254065513611,\n",
       "  73.72221946716309,\n",
       "  72.0634937286377,\n",
       "  73.4682559967041,\n",
       "  72.87301421165466,\n",
       "  73.06349277496338,\n",
       "  75.02381205558777,\n",
       "  75.71428418159485,\n",
       "  75.34920573234558,\n",
       "  76.70634984970093,\n",
       "  76.97619199752808,\n",
       "  76.65873169898987,\n",
       "  75.79365372657776,\n",
       "  77.68253684043884,\n",
       "  77.73016095161438,\n",
       "  78.20634841918945,\n",
       "  77.88095474243164,\n",
       "  76.87301635742188,\n",
       "  77.89682745933533,\n",
       "  78.13491821289062,\n",
       "  78.65872979164124,\n",
       "  78.32539677619934,\n",
       "  78.79365086555481,\n",
       "  78.1587302684784,\n",
       "  77.85714268684387,\n",
       "  78.39682698249817,\n",
       "  78.31745743751526,\n",
       "  78.49206328392029,\n",
       "  78.96825671195984,\n",
       "  77.96825170516968,\n",
       "  78.96825671195984,\n",
       "  79.54761981964111,\n",
       "  79.30952310562134,\n",
       "  79.41269874572754,\n",
       "  79.57936525344849,\n",
       "  80.03174662590027,\n",
       "  79.85714077949524,\n",
       "  79.97618913650513,\n",
       "  79.7936499118805,\n",
       "  79.52380776405334,\n",
       "  79.85714077949524,\n",
       "  79.7777771949768,\n",
       "  79.71428632736206,\n",
       "  80.06349205970764,\n",
       "  80.25397062301636,\n",
       "  79.3333351612091,\n",
       "  80.29364943504333,\n",
       "  80.63492178916931,\n",
       "  80.30158877372742,\n",
       "  80.37301301956177,\n",
       "  80.8571457862854,\n",
       "  80.19047379493713,\n",
       "  81.05555772781372,\n",
       "  80.69047331809998,\n",
       "  81.07143044471741,\n",
       "  80.650794506073,\n",
       "  80.98412752151489,\n",
       "  81.23809695243835,\n",
       "  80.88095188140869,\n",
       "  81.26190304756165,\n",
       "  81.11110925674438,\n",
       "  80.68253993988037,\n",
       "  81.18253946304321,\n",
       "  81.34127259254456,\n",
       "  81.0952365398407,\n",
       "  80.67460060119629,\n",
       "  80.7539701461792,\n",
       "  81.57142996788025,\n",
       "  80.37301301956177,\n",
       "  81.96031451225281,\n",
       "  81.69841170310974,\n",
       "  81.8412721157074,\n",
       "  80.6587278842926,\n",
       "  81.35714530944824,\n",
       "  81.57936334609985,\n",
       "  80.96031546592712,\n",
       "  81.39682412147522,\n",
       "  81.92856907844543,\n",
       "  81.94444179534912],\n",
       " 'val_f1': [0.7122005224227905,\n",
       "  0.7273060083389282,\n",
       "  0.7293963432312012,\n",
       "  0.7252671718597412,\n",
       "  0.7251318693161011,\n",
       "  0.7231386303901672,\n",
       "  0.7100807428359985,\n",
       "  0.7312778234481812,\n",
       "  0.7254611849784851,\n",
       "  0.7298187017440796,\n",
       "  0.7243160009384155,\n",
       "  0.7336055636405945,\n",
       "  0.7367457747459412,\n",
       "  0.735192060470581,\n",
       "  0.7388128638267517,\n",
       "  0.7310338020324707,\n",
       "  0.7446079850196838,\n",
       "  0.7325630187988281,\n",
       "  0.7334509491920471,\n",
       "  0.7217397689819336,\n",
       "  0.7397566437721252,\n",
       "  0.736130952835083,\n",
       "  0.7345097064971924,\n",
       "  0.7405462265014648,\n",
       "  0.7370522022247314,\n",
       "  0.7337566018104553,\n",
       "  0.7317342758178711,\n",
       "  0.7383971810340881,\n",
       "  0.7375800013542175,\n",
       "  0.7376354336738586,\n",
       "  0.7340143918991089,\n",
       "  0.7300173044204712,\n",
       "  0.7343065142631531,\n",
       "  0.7407981157302856,\n",
       "  0.7364962100982666,\n",
       "  0.7344861030578613,\n",
       "  0.7359848022460938,\n",
       "  0.739564836025238,\n",
       "  0.738382875919342,\n",
       "  0.7372280359268188,\n",
       "  0.73725426197052,\n",
       "  0.7255795001983643,\n",
       "  0.7352300882339478,\n",
       "  0.7283248901367188,\n",
       "  0.7338231801986694,\n",
       "  0.7506260871887207,\n",
       "  0.7595118284225464,\n",
       "  0.7447351813316345,\n",
       "  0.7666977643966675,\n",
       "  0.7709633111953735,\n",
       "  0.7687132358551025,\n",
       "  0.7623492479324341,\n",
       "  0.7782487869262695,\n",
       "  0.7794121503829956,\n",
       "  0.7798081040382385,\n",
       "  0.7810686826705933,\n",
       "  0.7725563049316406,\n",
       "  0.7808703184127808,\n",
       "  0.7829616069793701,\n",
       "  0.7877097725868225,\n",
       "  0.7799617052078247,\n",
       "  0.7872229814529419,\n",
       "  0.7844918370246887,\n",
       "  0.7818523645401001,\n",
       "  0.7829499244689941,\n",
       "  0.7844529747962952,\n",
       "  0.7874398827552795,\n",
       "  0.7913881540298462,\n",
       "  0.7833803296089172,\n",
       "  0.7920804023742676,\n",
       "  0.7977789640426636,\n",
       "  0.7957973480224609,\n",
       "  0.7963287234306335,\n",
       "  0.798454761505127,\n",
       "  0.8023061752319336,\n",
       "  0.7998801469802856,\n",
       "  0.8018405437469482,\n",
       "  0.8004676699638367,\n",
       "  0.7982721328735352,\n",
       "  0.8010709285736084,\n",
       "  0.8007790446281433,\n",
       "  0.8000257611274719,\n",
       "  0.8034160137176514,\n",
       "  0.8050591945648193,\n",
       "  0.7968617081642151,\n",
       "  0.8052599430084229,\n",
       "  0.807817816734314,\n",
       "  0.8053474426269531,\n",
       "  0.8062989711761475,\n",
       "  0.8104320764541626,\n",
       "  0.8046994209289551,\n",
       "  0.8127539753913879,\n",
       "  0.8095804452896118,\n",
       "  0.8126665949821472,\n",
       "  0.8091237545013428,\n",
       "  0.8122621774673462,\n",
       "  0.814543604850769,\n",
       "  0.8109194040298462,\n",
       "  0.8143981695175171,\n",
       "  0.8136282563209534,\n",
       "  0.8093069791793823,\n",
       "  0.8141127228736877,\n",
       "  0.8153990507125854,\n",
       "  0.8133316040039062,\n",
       "  0.8095687031745911,\n",
       "  0.810133695602417,\n",
       "  0.8174543976783752,\n",
       "  0.8067023754119873,\n",
       "  0.820981502532959,\n",
       "  0.8186737298965454,\n",
       "  0.8203960657119751,\n",
       "  0.8090523481369019,\n",
       "  0.8159204125404358,\n",
       "  0.817760705947876,\n",
       "  0.8125922083854675,\n",
       "  0.8163951635360718,\n",
       "  0.8203767538070679,\n",
       "  0.821030855178833],\n",
       " 'val_precision': [0.7105783224105835,\n",
       "  0.7261566519737244,\n",
       "  0.7291986346244812,\n",
       "  0.7292071580886841,\n",
       "  0.7311693429946899,\n",
       "  0.7279050350189209,\n",
       "  0.7300939559936523,\n",
       "  0.7302899956703186,\n",
       "  0.7329757809638977,\n",
       "  0.7300443649291992,\n",
       "  0.7264123558998108,\n",
       "  0.7333395481109619,\n",
       "  0.7354774475097656,\n",
       "  0.7360746264457703,\n",
       "  0.7393485307693481,\n",
       "  0.7356290817260742,\n",
       "  0.744387686252594,\n",
       "  0.7340123653411865,\n",
       "  0.7351890206336975,\n",
       "  0.7353107929229736,\n",
       "  0.7392665147781372,\n",
       "  0.736588716506958,\n",
       "  0.7354355454444885,\n",
       "  0.7394559383392334,\n",
       "  0.739773154258728,\n",
       "  0.7399857044219971,\n",
       "  0.7325516939163208,\n",
       "  0.737943172454834,\n",
       "  0.7370052337646484,\n",
       "  0.7417486906051636,\n",
       "  0.7365021109580994,\n",
       "  0.7308717966079712,\n",
       "  0.7342488169670105,\n",
       "  0.7414365410804749,\n",
       "  0.7355934381484985,\n",
       "  0.7342269420623779,\n",
       "  0.7367597222328186,\n",
       "  0.7387871742248535,\n",
       "  0.738276481628418,\n",
       "  0.7376693487167358,\n",
       "  0.7372865080833435,\n",
       "  0.7412159442901611,\n",
       "  0.7358487844467163,\n",
       "  0.7279532551765442,\n",
       "  0.7402908205986023,\n",
       "  0.7510547637939453,\n",
       "  0.7639994621276855,\n",
       "  0.7477551698684692,\n",
       "  0.766371488571167,\n",
       "  0.7726916074752808,\n",
       "  0.7726660370826721,\n",
       "  0.779455304145813,\n",
       "  0.7804981470108032,\n",
       "  0.783562183380127,\n",
       "  0.7790308594703674,\n",
       "  0.7857732772827148,\n",
       "  0.7854105830192566,\n",
       "  0.7844025492668152,\n",
       "  0.7857234477996826,\n",
       "  0.7893823385238647,\n",
       "  0.7796155214309692,\n",
       "  0.7866902351379395,\n",
       "  0.7922074794769287,\n",
       "  0.7916280031204224,\n",
       "  0.7822741866111755,\n",
       "  0.7864328026771545,\n",
       "  0.7934250831604004,\n",
       "  0.7945383191108704,\n",
       "  0.7967034578323364,\n",
       "  0.7977378964424133,\n",
       "  0.8032666444778442,\n",
       "  0.8031575679779053,\n",
       "  0.801328718662262,\n",
       "  0.8057646751403809,\n",
       "  0.8066699504852295,\n",
       "  0.8020778298377991,\n",
       "  0.8065420389175415,\n",
       "  0.8071982860565186,\n",
       "  0.8079553842544556,\n",
       "  0.8076709508895874,\n",
       "  0.8104857802391052,\n",
       "  0.8088204860687256,\n",
       "  0.8118350505828857,\n",
       "  0.8120157122612,\n",
       "  0.8107959032058716,\n",
       "  0.8111998438835144,\n",
       "  0.8105572462081909,\n",
       "  0.8113320469856262,\n",
       "  0.8136236667633057,\n",
       "  0.8145597577095032,\n",
       "  0.8133229613304138,\n",
       "  0.8184635043144226,\n",
       "  0.8178383111953735,\n",
       "  0.8172416090965271,\n",
       "  0.8169518709182739,\n",
       "  0.8191386461257935,\n",
       "  0.820166826248169,\n",
       "  0.8161168694496155,\n",
       "  0.8183336853981018,\n",
       "  0.8211896419525146,\n",
       "  0.8163327574729919,\n",
       "  0.8203597068786621,\n",
       "  0.8202290534973145,\n",
       "  0.8200403451919556,\n",
       "  0.8188714981079102,\n",
       "  0.8179041147232056,\n",
       "  0.8213321566581726,\n",
       "  0.8168511390686035,\n",
       "  0.8236771821975708,\n",
       "  0.8223936557769775,\n",
       "  0.8254302740097046,\n",
       "  0.8159663677215576,\n",
       "  0.8226279020309448,\n",
       "  0.8226025700569153,\n",
       "  0.8236548900604248,\n",
       "  0.8236042857170105,\n",
       "  0.8222471475601196,\n",
       "  0.8244318962097168],\n",
       " 'val_recall': [0.7150793671607971,\n",
       "  0.7289682626724243,\n",
       "  0.7296031713485718,\n",
       "  0.7228571772575378,\n",
       "  0.7219841480255127,\n",
       "  0.7203968167304993,\n",
       "  0.704444408416748,\n",
       "  0.732619047164917,\n",
       "  0.7219047546386719,\n",
       "  0.7296031713485718,\n",
       "  0.722777783870697,\n",
       "  0.7338888645172119,\n",
       "  0.7403968572616577,\n",
       "  0.7344444394111633,\n",
       "  0.7383333444595337,\n",
       "  0.7284126877784729,\n",
       "  0.7448412775993347,\n",
       "  0.7314285635948181,\n",
       "  0.7321428656578064,\n",
       "  0.7169841527938843,\n",
       "  0.7403174638748169,\n",
       "  0.735714316368103,\n",
       "  0.7337301969528198,\n",
       "  0.7422221899032593,\n",
       "  0.7352380752563477,\n",
       "  0.7306349277496338,\n",
       "  0.7310317158699036,\n",
       "  0.7439682483673096,\n",
       "  0.7382539510726929,\n",
       "  0.7352380752563477,\n",
       "  0.7323015928268433,\n",
       "  0.729285717010498,\n",
       "  0.7343651056289673,\n",
       "  0.7402380704879761,\n",
       "  0.7413492202758789,\n",
       "  0.7347618937492371,\n",
       "  0.7353174686431885,\n",
       "  0.7405555248260498,\n",
       "  0.7384920716285706,\n",
       "  0.7368254065513611,\n",
       "  0.7372222542762756,\n",
       "  0.720634937286377,\n",
       "  0.734682559967041,\n",
       "  0.7287302017211914,\n",
       "  0.7306349277496338,\n",
       "  0.7502381205558777,\n",
       "  0.7571428418159485,\n",
       "  0.7534920573234558,\n",
       "  0.7670634984970093,\n",
       "  0.7697619199752808,\n",
       "  0.7665872573852539,\n",
       "  0.7579364776611328,\n",
       "  0.7768253684043884,\n",
       "  0.777301549911499,\n",
       "  0.7820634841918945,\n",
       "  0.7788095474243164,\n",
       "  0.7687301635742188,\n",
       "  0.7789682745933533,\n",
       "  0.7813491821289062,\n",
       "  0.7865872979164124,\n",
       "  0.7832540273666382,\n",
       "  0.7879364490509033,\n",
       "  0.7815872430801392,\n",
       "  0.7785714268684387,\n",
       "  0.7839682102203369,\n",
       "  0.7831746339797974,\n",
       "  0.7849206328392029,\n",
       "  0.7896825075149536,\n",
       "  0.7796825170516968,\n",
       "  0.7896825075149536,\n",
       "  0.7954761981964111,\n",
       "  0.7930952310562134,\n",
       "  0.7941269874572754,\n",
       "  0.7957936525344849,\n",
       "  0.8003174662590027,\n",
       "  0.7985714673995972,\n",
       "  0.7997618913650513,\n",
       "  0.7979365587234497,\n",
       "  0.7952380776405334,\n",
       "  0.7985714673995972,\n",
       "  0.7977777719497681,\n",
       "  0.7971428632736206,\n",
       "  0.8006349205970764,\n",
       "  0.8025397062301636,\n",
       "  0.7933332920074463,\n",
       "  0.8029364943504333,\n",
       "  0.8063492178916931,\n",
       "  0.8030158877372742,\n",
       "  0.8037301898002625,\n",
       "  0.808571457862854,\n",
       "  0.8019047975540161,\n",
       "  0.8105555176734924,\n",
       "  0.8069047331809998,\n",
       "  0.8107142448425293,\n",
       "  0.80650794506073,\n",
       "  0.8098412752151489,\n",
       "  0.8123809695243835,\n",
       "  0.8088095188140869,\n",
       "  0.8126190900802612,\n",
       "  0.8111111521720886,\n",
       "  0.8068253993988037,\n",
       "  0.8118253946304321,\n",
       "  0.8134127259254456,\n",
       "  0.8109524250030518,\n",
       "  0.8067460656166077,\n",
       "  0.807539701461792,\n",
       "  0.8157142996788025,\n",
       "  0.8037301301956177,\n",
       "  0.8196032047271729,\n",
       "  0.8169841170310974,\n",
       "  0.818412721157074,\n",
       "  0.806587278842926,\n",
       "  0.8135714530944824,\n",
       "  0.8157936334609985,\n",
       "  0.809603214263916,\n",
       "  0.8139682412147522,\n",
       "  0.8192856907844543,\n",
       "  0.8194444179534912],\n",
       " 'val_auroc': [0.7521320581436157,\n",
       "  0.7656793594360352,\n",
       "  0.7749887704849243,\n",
       "  0.7740785479545593,\n",
       "  0.7805913686752319,\n",
       "  0.770334005355835,\n",
       "  0.7805086970329285,\n",
       "  0.7770739793777466,\n",
       "  0.784277617931366,\n",
       "  0.7717479467391968,\n",
       "  0.7723918557167053,\n",
       "  0.7803111672401428,\n",
       "  0.7838531732559204,\n",
       "  0.7879112958908081,\n",
       "  0.7915553450584412,\n",
       "  0.7934503555297852,\n",
       "  0.7967376708984375,\n",
       "  0.7804796695709229,\n",
       "  0.7818963527679443,\n",
       "  0.7872763872146606,\n",
       "  0.7932205200195312,\n",
       "  0.794036865234375,\n",
       "  0.784091591835022,\n",
       "  0.7889710664749146,\n",
       "  0.7945214509963989,\n",
       "  0.7903704643249512,\n",
       "  0.7826037406921387,\n",
       "  0.7885675430297852,\n",
       "  0.7884061336517334,\n",
       "  0.7874456644058228,\n",
       "  0.7860656976699829,\n",
       "  0.7834046483039856,\n",
       "  0.7843350172042847,\n",
       "  0.789654016494751,\n",
       "  0.790966272354126,\n",
       "  0.787693977355957,\n",
       "  0.7872066497802734,\n",
       "  0.7886914610862732,\n",
       "  0.7834077477455139,\n",
       "  0.7838846445083618,\n",
       "  0.7881721258163452,\n",
       "  0.7895747423171997,\n",
       "  0.7897375822067261,\n",
       "  0.7762875556945801,\n",
       "  0.7960336208343506,\n",
       "  0.8035497665405273,\n",
       "  0.8163230419158936,\n",
       "  0.8191273808479309,\n",
       "  0.8296065330505371,\n",
       "  0.830791711807251,\n",
       "  0.8348652720451355,\n",
       "  0.8385514616966248,\n",
       "  0.8442636728286743,\n",
       "  0.8441922664642334,\n",
       "  0.8435394763946533,\n",
       "  0.8492768406867981,\n",
       "  0.8448455333709717,\n",
       "  0.8497589230537415,\n",
       "  0.850236177444458,\n",
       "  0.8513455986976624,\n",
       "  0.8417422771453857,\n",
       "  0.8519185781478882,\n",
       "  0.8539208769798279,\n",
       "  0.8550902009010315,\n",
       "  0.8521138429641724,\n",
       "  0.8491711616516113,\n",
       "  0.8529822826385498,\n",
       "  0.861929178237915,\n",
       "  0.8591948747634888,\n",
       "  0.8630568385124207,\n",
       "  0.865699291229248,\n",
       "  0.8691074252128601,\n",
       "  0.8683674931526184,\n",
       "  0.8705276250839233,\n",
       "  0.8709787130355835,\n",
       "  0.8689815402030945,\n",
       "  0.873178243637085,\n",
       "  0.8712555170059204,\n",
       "  0.8732585310935974,\n",
       "  0.8738534450531006,\n",
       "  0.8736156821250916,\n",
       "  0.8754370808601379,\n",
       "  0.8782804012298584,\n",
       "  0.8779828548431396,\n",
       "  0.879356861114502,\n",
       "  0.877638578414917,\n",
       "  0.8777637481689453,\n",
       "  0.876956582069397,\n",
       "  0.8785690069198608,\n",
       "  0.8809692859649658,\n",
       "  0.8818050026893616,\n",
       "  0.8839165568351746,\n",
       "  0.8839544057846069,\n",
       "  0.8819530010223389,\n",
       "  0.8821830749511719,\n",
       "  0.8827880620956421,\n",
       "  0.8848004341125488,\n",
       "  0.8833498358726501,\n",
       "  0.8830664753913879,\n",
       "  0.8871599435806274,\n",
       "  0.8855040073394775,\n",
       "  0.8868110179901123,\n",
       "  0.8854675889015198,\n",
       "  0.8881796598434448,\n",
       "  0.8865698575973511,\n",
       "  0.8867675065994263,\n",
       "  0.8902730941772461,\n",
       "  0.8846901059150696,\n",
       "  0.8903292417526245,\n",
       "  0.8907166719436646,\n",
       "  0.8926214575767517,\n",
       "  0.8847304582595825,\n",
       "  0.8916538953781128,\n",
       "  0.8906549215316772,\n",
       "  0.8924038410186768,\n",
       "  0.8900070190429688,\n",
       "  0.8883748054504395,\n",
       "  0.8903239965438843],\n",
       " 'epoch': [1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T23:21:36.890547Z",
     "start_time": "2025-08-02T23:21:36.883242Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), model_save_path)",
   "id": "11ea084fbb2c0751",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:13:26.409897Z",
     "start_time": "2025-07-24T10:42:36.658857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_save_path = \"dynamic_gin_embedding_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "trained_model, history = simple_train_model_v2(\n",
    "        dataset_trim,\n",
    "        gnn_model=model,\n",
    "        num_epochs=5,\n",
    "        batch_size=4,\n",
    "        learning_rate=0.002,\n",
    "        start_index=0,\n",
    "        num_graphs_to_use=60000,\n",
    "    )"
   ],
   "id": "346ebeb4d2811d0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 1: 38182 instances\n",
      "Label 0: 21818 instances\n",
      "Label 1: 63.64% of total instances\n",
      "Label 0: 36.36% of total instances\n",
      "Class weights: tensor([2.7500, 1.5714], device='cuda:0')\n",
      "Splitting dataset into train and validation sets\n",
      "Train samples: 48000\n",
      "Validation samples: 12000\n",
      "\n",
      "Starting training for 5 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|| 12000/12000 [04:33<00:00, 43.87it/s, loss=0.2671, acc=81.23%]\n",
      "Epoch 1/5 [Val]: 100%|| 3000/3000 [00:50<00:00, 59.09it/s, loss=0.6085, acc=81.62%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 0.4351, Acc: 81.23% | Val Loss: 0.4540, Acc: 81.62% (Best Val: 81.62%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|| 12000/12000 [05:10<00:00, 38.67it/s, loss=0.0419, acc=81.33%]\n",
      "Epoch 2/5 [Val]: 100%|| 3000/3000 [02:21<00:00, 21.18it/s, loss=0.5406, acc=82.01%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Train Loss: 0.4342, Acc: 81.33% | Val Loss: 0.4338, Acc: 82.01% (Best Val: 82.01%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]: 100%|| 12000/12000 [04:28<00:00, 44.69it/s, loss=0.1811, acc=81.44%] \n",
      "Epoch 3/5 [Val]: 100%|| 3000/3000 [00:51<00:00, 58.78it/s, loss=0.6834, acc=81.19%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Train Loss: 0.4330, Acc: 81.44% | Val Loss: 0.4391, Acc: 81.19% (Best Val: 82.01%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|| 12000/12000 [05:01<00:00, 39.86it/s, loss=0.2786, acc=81.44%]\n",
      "Epoch 4/5 [Val]: 100%|| 3000/3000 [02:22<00:00, 21.02it/s, loss=0.5135, acc=82.33%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Train Loss: 0.4297, Acc: 81.44% | Val Loss: 0.4412, Acc: 82.33% (Best Val: 82.33%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Train]: 100%|| 12000/12000 [04:19<00:00, 46.19it/s, loss=0.3382, acc=81.76%]\n",
      "Epoch 5/5 [Val]: 100%|| 3000/3000 [00:49<00:00, 60.11it/s, loss=0.3723, acc=80.85%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Train Loss: 0.4255, Acc: 81.76% | Val Loss: 0.4389, Acc: 80.85% (Best Val: 82.33%)\n",
      "Training completed!\n",
      "Training time: 1849.739828900012\n",
      "Best validation accuracy: 82.33%\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:52:13.292266Z",
     "start_time": "2025-08-20T19:52:13.283466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "import optuna\n",
    "optuna_result = r\"optuna_results_20250820_213139.pkl\"\n",
    "with open(optuna_result, \"rb\") as f:\n",
    "    optuna_results = joblib.load(f)"
   ],
   "id": "6ebb8ee5b7af1a5c",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:41:04.667067Z",
     "start_time": "2025-08-20T19:41:04.659996Z"
    }
   },
   "cell_type": "code",
   "source": "[item.values for item in optuna_results[\"fold_0\"][\"study\"].trials]",
   "id": "8da2ff870dace0b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5],\n",
       " [0.5],\n",
       " [0.9350761771202087],\n",
       " [0.5],\n",
       " [0.9295566082000732],\n",
       " [0.9362394213676453],\n",
       " [0.9269653558731079],\n",
       " [0.9116346836090088],\n",
       " [0.5],\n",
       " [0.93639075756073],\n",
       " [0.9339721202850342],\n",
       " [0.93220055103302],\n",
       " [0.9363347887992859],\n",
       " [0.9348156452178955],\n",
       " [0.9363428354263306],\n",
       " [0.9329842925071716],\n",
       " [0.9324948787689209],\n",
       " [0.9326227307319641],\n",
       " [0.9329482913017273],\n",
       " [0.934323787689209],\n",
       " [0.9346429109573364],\n",
       " [0.9260116815567017],\n",
       " [0.9329156875610352],\n",
       " [0.9320093393325806],\n",
       " [0.9302478432655334],\n",
       " [0.9287922382354736],\n",
       " [0.9238104820251465],\n",
       " [0.9318323731422424],\n",
       " [0.9344959259033203],\n",
       " [0.9293798208236694],\n",
       " [0.9314045310020447],\n",
       " [0.9294335842132568],\n",
       " [0.9345998764038086],\n",
       " [0.9342455863952637],\n",
       " [0.9339947700500488],\n",
       " [0.9317659139633179],\n",
       " [0.9368284940719604],\n",
       " [0.933342456817627],\n",
       " [0.9371731281280518],\n",
       " [0.9343671798706055],\n",
       " [0.929416298866272],\n",
       " [0.9372449517250061],\n",
       " [0.9312037229537964],\n",
       " [0.9377567172050476],\n",
       " [0.9371228218078613],\n",
       " [0.937757134437561],\n",
       " [0.9361299276351929],\n",
       " [0.9372646808624268],\n",
       " [0.9328896999359131],\n",
       " [0.9369758367538452]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:41:38.516326Z",
     "start_time": "2025-08-20T19:41:38.511602Z"
    }
   },
   "cell_type": "code",
   "source": "optuna_results",
   "id": "280838a4b64be736",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fold_0': {'best_params': {'num_layers': 1,\n",
       "   'hidden_size': 256,\n",
       "   'conv_dropout_rate': 0.3,\n",
       "   'classifier_dropout_rate': 0.1,\n",
       "   'use_layer_norm': False,\n",
       "   'pool_hidden_size': 256,\n",
       "   'num_epochs': 200,\n",
       "   'batch_size': 32,\n",
       "   'learning_rate': 0.00016847952401702993,\n",
       "   'optimizer_scheduler': 'ReduceLROnPlateau'},\n",
       "  'best_score': 0.937757134437561,\n",
       "  'study': <optuna.study.study.Study at 0x2364c6d1c10>}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:51:33.938921Z",
     "start_time": "2025-08-20T19:51:33.936236Z"
    }
   },
   "cell_type": "code",
   "source": "pickled_study = optuna_results[\"fold_0\"][\"study\"]",
   "id": "de0f7c68dfc5a058",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:59:31.109452Z",
     "start_time": "2025-08-20T19:59:31.106428Z"
    }
   },
   "cell_type": "code",
   "source": "pickled_study.study_name",
   "id": "cfdca3e033802d3d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no-name-f04c1bde-e879-4d38-9aee-bb8a7454aaee'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T20:01:39.089891Z",
     "start_time": "2025-08-20T20:01:38.555762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Loaded study with {len(pickled_study.trials)} trials\")\n",
    "print(f\"Best value: {pickled_study.best_value}\")\n",
    "\n",
    "# Create a new study with SQLite storage\n",
    "sqlite_study = optuna.create_study(\n",
    "    study_name=\"gine_study_fold_00\",\n",
    "    storage=\"sqlite:///optuna_gine_20252008_215100.db\",\n",
    "    direction=\"maximize\",  # or \"minimize\" depending on your original study\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# Copy all trials from pickled study to SQLite study\n",
    "for trial in pickled_study.trials:\n",
    "    # Add each trial to the new study\n",
    "    sqlite_study.add_trial(trial)\n",
    "\n",
    "print(f\" Converted {len(sqlite_study.trials)} trials to SQLite\")\n",
    "print(f\"Best value in SQLite study: {sqlite_study.best_value}\")"
   ],
   "id": "8c7486d5ec64191a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 22:01:38,586] A new study created in RDB with name: gine_study_fold_00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded study with 50 trials\n",
      "Best value: 0.937757134437561\n",
      " Converted 50 trials to SQLite\n",
      "Best value in SQLite study: 0.937757134437561\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T20:02:37.520474Z",
     "start_time": "2025-08-20T20:02:36.165355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "optuna.importance.get_param_importances(pickled_study)"
   ],
   "id": "39e133bd87edc792",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.7748953462439422,\n",
       " 'use_layer_norm': 0.06467817437922377,\n",
       " 'hidden_size': 0.03659707765069031,\n",
       " 'batch_size': 0.035762766894939874,\n",
       " 'classifier_dropout_rate': 0.02626405077028361,\n",
       " 'conv_dropout_rate': 0.024877122686233894,\n",
       " 'num_layers': 0.01748525653032421,\n",
       " 'num_epochs': 0.013675642188101328,\n",
       " 'pool_hidden_size': 0.005764199542641594,\n",
       " 'optimizer_scheduler': 3.6311361930541533e-07}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T20:02:14.800917Z",
     "start_time": "2025-08-20T20:02:14.796346Z"
    }
   },
   "cell_type": "code",
   "source": "pickled_study.best_params",
   "id": "ef7cdeb056d0e506",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_layers': 1,\n",
       " 'hidden_size': 256,\n",
       " 'conv_dropout_rate': 0.3,\n",
       " 'classifier_dropout_rate': 0.1,\n",
       " 'use_layer_norm': False,\n",
       " 'pool_hidden_size': 256,\n",
       " 'num_epochs': 200,\n",
       " 'batch_size': 32,\n",
       " 'learning_rate': 0.00016847952401702993,\n",
       " 'optimizer_scheduler': 'ReduceLROnPlateau'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T10:23:02.762589Z",
     "start_time": "2025-08-22T10:23:02.717821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import dill\n",
    "\n",
    "class MockTensor:\n",
    "    def __init__(self, data, dtype=None, device=None, requires_grad=False):\n",
    "        if isinstance(data, np.ndarray):\n",
    "            self.data = data\n",
    "        else:\n",
    "            self.data = np.array(data)\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.requires_grad = requires_grad\n",
    "        self.shape = self.data.shape\n",
    "        self.grad = None\n",
    "\n",
    "    def numpy(self):\n",
    "        return self.data\n",
    "\n",
    "    def cpu(self):\n",
    "        return MockTensor(self.data, self.dtype, 'cpu', self.requires_grad)\n",
    "\n",
    "    def cuda(self):\n",
    "        return MockTensor(self.data, self.dtype, 'cuda', self.requires_grad)\n",
    "\n",
    "    def detach(self):\n",
    "        return MockTensor(self.data, self.dtype, self.device, False)\n",
    "\n",
    "    def clone(self):\n",
    "        return MockTensor(self.data.copy(), self.dtype, self.device, self.requires_grad)\n",
    "\n",
    "    def __array__(self):\n",
    "        return self.data\n",
    "\n",
    "    def size(self, dim=None):\n",
    "        if dim is None:\n",
    "            return self.shape\n",
    "        return self.shape[dim]\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return MockTensor(self.data[key], self.dtype, self.device, self.requires_grad)\n",
    "\n",
    "class MockStorage:\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.data[key]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class MockTorch:\n",
    "    Tensor = MockTensor\n",
    "\n",
    "    # Mock data types\n",
    "    float32 = np.float32\n",
    "    float64 = np.float64\n",
    "    int32 = np.int32\n",
    "    int64 = np.int64\n",
    "    uint8 = np.uint8\n",
    "\n",
    "    # Mock devices\n",
    "    class device:\n",
    "        def __init__(self, device_str):\n",
    "            self.type = device_str\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor(data, dtype=None, device=None, requires_grad=False):\n",
    "        return MockTensor(data, dtype, device, requires_grad)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_numpy(arr):\n",
    "        return MockTensor(arr)\n",
    "\n",
    "    # Critical: Mock the internal tensor reconstruction functions\n",
    "    @staticmethod\n",
    "    def _rebuild_tensor_v2(storage, storage_offset, size, stride, requires_grad, backward_hooks):\n",
    "        \"\"\"This is the function PyTorch uses internally to rebuild tensors from pickle\"\"\"\n",
    "        # Convert storage to numpy array\n",
    "        if hasattr(storage, 'data'):\n",
    "            data = storage.data\n",
    "        else:\n",
    "            data = np.array(storage)\n",
    "\n",
    "        # Reshape according to size\n",
    "        try:\n",
    "            if len(size) > 0:\n",
    "                data = data.reshape(size)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return MockTensor(data, requires_grad=requires_grad)\n",
    "\n",
    "    @staticmethod\n",
    "    def _rebuild_tensor(storage, storage_offset, size, stride):\n",
    "        \"\"\"Older version of tensor rebuild function\"\"\"\n",
    "        if hasattr(storage, 'data'):\n",
    "            data = storage.data\n",
    "        else:\n",
    "            data = np.array(storage)\n",
    "\n",
    "        try:\n",
    "            if len(size) > 0:\n",
    "                data = data.reshape(size)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return MockTensor(data)\n",
    "\n",
    "    # Mock storage types\n",
    "    @staticmethod\n",
    "    def FloatStorage(*args, **kwargs):\n",
    "        return MockStorage(np.array(args[0] if args else [], dtype=np.float32))\n",
    "\n",
    "    @staticmethod\n",
    "    def DoubleStorage(*args, **kwargs):\n",
    "        return MockStorage(np.array(args[0] if args else [], dtype=np.float64))\n",
    "\n",
    "    @staticmethod\n",
    "    def LongStorage(*args, **kwargs):\n",
    "        return MockStorage(np.array(args[0] if args else [], dtype=np.int64))\n",
    "\n",
    "    @staticmethod\n",
    "    def IntStorage(*args, **kwargs):\n",
    "        return MockStorage(np.array(args[0] if args else [], dtype=np.int32))\n",
    "\n",
    "# Create the mock modules\n",
    "torch_mock = MockTorch()\n",
    "sys.modules['torch'] = torch_mock\n",
    "\n",
    "# Also add the rebuild functions to the torch module directly\n",
    "torch_mock._rebuild_tensor_v2 = MockTorch._rebuild_tensor_v2\n",
    "torch_mock._rebuild_tensor = MockTorch._rebuild_tensor\n",
    "\n",
    "# Mock other common PyTorch modules\n",
    "sys.modules['torch.nn'] = type(sys)('torch.nn')\n",
    "sys.modules['torch.nn.functional'] = type(sys)('torch.nn.functional')\n",
    "sys.modules['torch_geometric'] = type(sys)('torch_geometric')\n",
    "\n",
    "# Now try loading your pickle file\n",
    "try:\n",
    "    with open('all_folds_best_params_results_20250822_022835_dill.pkl', 'rb') as f:\n",
    "        data = dill.load(f)\n",
    "    print(\"Successfully loaded!\")\n",
    "    print(f\"Data type: {type(data)}\")\n",
    "\n",
    "    # You can now access tensor data as NumPy arrays\n",
    "    # For example, if data contains tensors:\n",
    "    # numpy_array = some_tensor.numpy()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    # If you still get errors, print them so we can add more mocks"
   ],
   "id": "cbcecbc91046314",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Can't get attribute '_rebuild_tensor_v2' on <__main__.MockTorch object at 0x000001AF998F5910>\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T10:20:57.348828Z",
     "start_time": "2025-08-22T10:20:57.262233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from safetensors.numpy import save_file, load_file\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "class MockTensor:\n",
    "    def __init__(self, data, dtype=None, device=None):\n",
    "        if isinstance(data, np.ndarray):\n",
    "            self.data = data\n",
    "        else:\n",
    "            self.data = np.array(data)\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.shape = self.data.shape\n",
    "\n",
    "    def numpy(self):\n",
    "        return self.data\n",
    "\n",
    "    def cpu(self):\n",
    "        return MockTensor(self.data, self.dtype, 'cpu')\n",
    "\n",
    "    def cuda(self):\n",
    "        return MockTensor(self.data, self.dtype, 'cuda')\n",
    "\n",
    "    def detach(self):\n",
    "        return self\n",
    "\n",
    "    def clone(self):\n",
    "        return MockTensor(self.data.copy(), self.dtype, self.device)\n",
    "\n",
    "    def __array__(self):\n",
    "        return self.data\n",
    "\n",
    "class MockTorch:\n",
    "    Tensor = MockTensor\n",
    "    float32 = np.float32\n",
    "    float64 = np.float64\n",
    "    int32 = np.int32\n",
    "    int64 = np.int64\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor(data, dtype=None):\n",
    "        return MockTensor(data, dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_numpy(arr):\n",
    "        return MockTensor(arr)\n",
    "\n",
    "\n",
    "# Mock torch and related modules\n",
    "sys.modules['torch'] = MockTorch()\n",
    "sys.modules['torch.nn'] = MockTorch()\n",
    "sys.modules['torch.nn.functional'] = MockTorch()\n",
    "sys.modules['torch_geometric'] = MockTorch()\n",
    "sys.modules['torch._utils'] = MockTorch()\n",
    "sys.modules['torch.storage'] = MockTorch()\n",
    "\n",
    "import dill\n",
    "all_results = {}\n",
    "for fold in range(1,10):\n",
    "    with open(rf\"fold_{fold}results.pkl\", \"rb\") as f:\n",
    "        fold_results = dill.load(f)\n",
    "    all_results[f\"fold_{fold}\"] = fold_results"
   ],
   "id": "532ee2be29adb568",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute '_rebuild_tensor_v2' on <__main__.MockTorch object at 0x000001AF998F5910>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 62\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fold \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m10\u001B[39m):\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124mrf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfold_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mresults.pkl\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m---> 62\u001B[0m         fold_results \u001B[38;5;241m=\u001B[39m dill\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[0;32m     63\u001B[0m     all_results[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfold_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m fold_results\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\dill\\_dill.py:297\u001B[0m, in \u001B[0;36mload\u001B[1;34m(file, ignore, **kwds)\u001B[0m\n\u001B[0;32m    291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(file, ignore\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[0;32m    292\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;124;03m    Unpickle an object from a file.\u001B[39;00m\n\u001B[0;32m    294\u001B[0m \n\u001B[0;32m    295\u001B[0m \u001B[38;5;124;03m    See :func:`loads` for keyword arguments.\u001B[39;00m\n\u001B[0;32m    296\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 297\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Unpickler(file, ignore\u001B[38;5;241m=\u001B[39mignore, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\u001B[38;5;241m.\u001B[39mload()\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\dill\\_dill.py:452\u001B[0m, in \u001B[0;36mUnpickler.load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    451\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;28mself\u001B[39m): \u001B[38;5;66;03m#NOTE: if settings change, need to update attributes\u001B[39;00m\n\u001B[1;32m--> 452\u001B[0m     obj \u001B[38;5;241m=\u001B[39m StockUnpickler\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    453\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(obj)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(_main_module, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__name__\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m    454\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ignore:\n\u001B[0;32m    455\u001B[0m             \u001B[38;5;66;03m# point obj class to main\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\dill\\_dill.py:442\u001B[0m, in \u001B[0;36mUnpickler.find_class\u001B[1;34m(self, module, name)\u001B[0m\n\u001B[0;32m    440\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;66;03m#XXX: special case: NoneType missing\u001B[39;00m\n\u001B[0;32m    441\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m module \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdill.dill\u001B[39m\u001B[38;5;124m'\u001B[39m: module \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdill._dill\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 442\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m StockUnpickler\u001B[38;5;241m.\u001B[39mfind_class(\u001B[38;5;28mself\u001B[39m, module, name)\n",
      "\u001B[1;31mAttributeError\u001B[0m: Can't get attribute '_rebuild_tensor_v2' on <__main__.MockTorch object at 0x000001AF998F5910>"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T10:13:57.316819Z",
     "start_time": "2025-08-22T10:13:57.305319Z"
    }
   },
   "cell_type": "code",
   "source": "fold_results",
   "id": "6a0659060598ca99",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_params': {'num_layers': 1,\n",
       "  'hidden_size': 256,\n",
       "  'conv_dropout_rate': 0.3,\n",
       "  'classifier_dropout_rate': 0.1,\n",
       "  'use_layer_norm': False,\n",
       "  'pool_hidden_size': 256,\n",
       "  'num_epochs': 200,\n",
       "  'batch_size': 32,\n",
       "  'learning_rate': 0.00016847952401702993,\n",
       "  'optimizer_scheduler': 'ReduceLROnPlateau'},\n",
       " 'train_tracker': {'acc': <MagicMock name='mock._rebuild_tensor_v2()' id='1853740023680'>,\n",
       "  'auroc': <MagicMock name='mock._rebuild_tensor_v2()' id='1853740023680'>,\n",
       "  'f1': <MagicMock name='mock._rebuild_tensor_v2()' id='1853740023680'>},\n",
       " 'val_tracker': {'acc': <MagicMock name='mock._rebuild_tensor_v2()' id='1853740023680'>,\n",
       "  'auroc': <MagicMock name='mock._rebuild_tensor_v2()' id='1853740023680'>,\n",
       "  'f1': <MagicMock name='mock._rebuild_tensor_v2()' id='1853740023680'>}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
